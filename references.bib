@article{Biedert2018,
   abstract = {The physical separation of compute resource and end user is one of the core challenges in HPC visualization. While GPUs are routinely used in remote rendering, a heretofore unexplored aspect is these GPUs' special purpose video en-/decoding hardware that can be used to solve the large-scale remoting challenge. The high performance and substantial bandwidth savings offered by such hardware enable a novel approach to the problems inherent in remote rendering, with impact on the workflows and visualization scenarios available. Using more tiles than previously thought reasonable, we demonstrate a distributed, low-latency multi-tile streaming system that can sustain stable 80 Hz when streaming up to 256 synchronized 3840x2160 tiles and can achieve 365 Hz at 3840x2160 for sort-first compositing over the internet.},
   author = {T Biedert and P Messmer and T Fogal and C Garth},
   doi = {10.2312/pgv.20181093},
   journal = {Eurographics Symposium on Parallel Graphics and Visualization},
   keywords = {Graphics Systems-Distributed/network graphics,I32 [Computer Graphics]},
   title = {Hardware-Accelerated Multi-Tile Streaming for Realtime Remote Visualization},
   url = {https://diglib.eg.org},
   year = {2018},
}
@article{Stegmaier2002,
   abstract = {This paper presents a generic solution for hardware-accelerated remote visualization that works transparently for all OpenGL-based applications and OpenGL-based scene graphs. Universality is achieved by taking advantage of dynamic linking, efficient data transfer by means of VNC. The proposed solution does not require any modifications of existing applications and allows for remote visualization with different hardware architectures involved in the visualization process. The library's performance is evaluated using standard OpenGL example programs and by volume rendering substantial data sets.},
   author = {Simon Stegmaier and Marcelo Magallón and Thomas Ertl},
   keywords = {C24 [Distributed Systems],Distributed applications,Distributed/network graphics,I31 [Computer Graphics]},
   title = {A Generic Solution for Hardware-Accelerated Remote Visualization},
   url = {http://www.tgs.com},
   year = {2002},
}
@article{Lamberti2007,
  author={Lamberti, Fabrizio and Sanna, Andrea},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={A Streaming-Based Solution for Remote Visualization of 3D Graphics on Mobile Devices}, 
  year={2007},
  volume={13},
  number={2},
  pages={247-260},
  doi={10.1109/TVCG.2007.29}
}
@INPROCEEDINGS{Stegmaier2003,
  author={Stegmaier, S. and Diepstraten, J. and Weiler, M. and Ertl, T.},
  booktitle={3rd International Symposium on Image and Signal Processing and Analysis, 2003. ISPA 2003. Proceedings of the}, 
  title={Widening the remote visualization bottleneck}, 
  year={2003},
  volume={1},
  number={},
  pages={174-179 Vol.1},
  doi={10.1109/ISPA.2003.1296889}
}
@article{Pacheco2021,
   abstract = {Citation: Pacheco-Gutierrez, S.; Niu, H.; Caliskanelli, I.; Skilton, R.},
   author = {Salvador Pacheco-Gutierrez and Hanlin Niu and Ipek Caliskanelli and Robert Skilton},
   doi = {10.3390/robotics10030089},
   title = {robotics A Multiple Level-of-Detail 3D Data Transmission Approach for Low-Latency Remote Visualisation in Teleoperation Tasks A Multiple Level-of-Detail 3D Data Transmission Approach for},
   url = {https://doi.org/10.3390/robotics10030089},
   year = {2021},
}
@article{Lamberti2007,
   abstract = {Mobile devices such as Personal Digital Assistants, Tablet PCs, and cellular phones have greatly enhanced user capability to connect to remote resources. Although a large set of applications are now available bridging the gap between desktop and mobile devices, visualization of complex 3D models is still a task hard to accomplish without specialized hardware. This paper proposes a system where a cluster of PCs, equipped with accelerated graphics cards managed by the Chromium software, is able to handle remote visualization sessions based on MPEG video streaming involving complex 3D models. The proposed framework allows mobile devices such as smart phones, Personal Digital Assistants (PDAs), and Tablet PCs to visualize objects consisting of millions of textured polygons and voxels at a frame rate of 30 fps or more depending on hardware resources at the server side and on multimedia capabilities at the client side. The server is able to concurrently manage multiple clients computing a video stream for each one; resolution and quality of each stream is tailored according to screen resolution and bandwidth of the client. The paper investigates in depth issues related to latency time, bit rate and quality of the generated stream, screen resolutions, as well as frames per second displayed. © 2007 IEEE.},
   author = {Fabrizio Lamberti and Andrea Sanna},
   doi = {10.1109/TVCG.2007.29},
   issn = {10772626},
   issue = {2},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   keywords = {Chromium,Cluster-based rendering,MPEG,Mobile devices,Remote visualization},
   month = {3},
   pages = {247-260},
   pmid = {17218742},
   title = {A streaming-based solution for remote visualization of 3D graphics on mobile devices},
   volume = {13},
   year = {2007},
}
@inproceedings{Freitag2001,
   abstract = {We compare three remote visualization strategies used for interactive exploration of large data sets: image-based rendering , parallel visualization servers, and subsampling. We review each strategy and provide details for an adaptive multiresolu-tion subsampling technique that we have developed. To determine the problem regimes for which each approach is most cost effective, we develop performance models to analyze the costs of computation and communication associated with the common visualization task of isosurface generation. Using these models, we investigate a number of hardware system configurations and task complexity scenarios when parameters such as problem size, visualization demands, and network bandwidth change. For one particular strategy, subsampling, we further investigate the tradeoffs between multiresolution and uniform grid methods in terms of performance and approximation errors .},
   author = {L.A. Freitag and R.M. Loy},
   doi = {10.1109/IPDPS.2001.925184},
   isbn = {0-7695-0990-8},
   journal = {Proceedings 15th International Parallel and Distributed Processing Symposium. IPDPS 2001},
   keywords = {Interactive Visualization,Large Data Set Exploration,Performance Models,Remote Visualization},
   pages = {1915-1922},
   publisher = {IEEE Comput. Soc},
   title = {Comparison of remote visualization strategies for interactive exploration of large data sets},
   url = {http://ieeexplore.ieee.org/document/925184/},
}
@article{Ewins1998,
   author = {J.P. Ewins and M.D. Waller and M. White and P.F. Lister},
   doi = {10.1109/2945.765326},
   issn = {10772626},
   issue = {4},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   pages = {317-329},
   title = {MIP-map level selection for texture mapping},
   volume = {4},
   url = {http://ieeexplore.ieee.org/document/765326/},
   year = {1998},
}
@article{Dado2016,
   abstract = {Voxel-based approaches are today's standard to encode volume data. Recently, directed acyclic graphs (DAGs) were successfully used for compressing sparse voxel scenes as well, but they are restricted to a single bit of (geometry) information per voxel. We present a method to compress arbitrary data, such as colors, normals, or reflectance information. By decoupling geometry and voxel data via a novel mapping scheme, we are able to apply the DAG principle to encode the topology, while using a palette-based compression for the voxel attributes, leading to a drastic memory reduction. Our method outperforms existing state-of-the-art techniques and is well-suited for GPU architectures. We achieve real-time performance on commodity hardware for colored scenes with up to 17 hierarchical levels (a 128K3voxel resolution), which are stored fully in core.},
   author = {Bas Dado and Timothy R. Kol and Pablo Bauszat and Jean-Marc Thiery and Elmar Eisemann},
   doi = {10.1111/cgf.12841},
   issn = {01677055},
   issue = {2},
   journal = {Computer Graphics Forum},
   month = {5},
   pages = {397-407},
   title = {Geometry and Attribute Compression for Voxel Scenes},
   volume = {35},
   url = {http://doi.wiley.com/10.1111/cgf.12841},
   year = {2016},
}
@article{Baracaglia2019,
   abstract = {Virtual Reality (VR) technology has been subject to a rapid democratization in recent years, driven in large by the entertainment industry, and epitomized by the emergence of consumer-grade, plug-and-play, room-scale VR devices. To explore the scientific potential of this technology for the field of observational astrophysics, we have created an experimental VR application: E0102-VR. The specific scientific goal of this application is to facilitate the characterization of the 3D structure of the oxygen-rich ejecta in the young supernova remnant 1E 0102.2-7219 in the Small Magellanic Cloud. Using E0102-VR, we measure the physical size of two large cavities in the system, including a (7.0$\pm$0.5) pc-long funnel structure on the far-side of the remnant. The E0102-VR application, albeit experimental, demonstrates the benefits of using human depth perception for a rapid and accurate characterization of complex 3D structures. Given the implementation costs (time-wise) of a dedicated VR application like E0102-VR, we conclude that the future of VR for scientific purposes in astrophysics most likely resides in the development of a robust, generic application dedicated to the exploration and visualization of 3D observational datasets, akin to a ``ds9-VR''.},
   author = {E. Baracaglia and F. P. A. Vogt},
   keywords = {ISM: individual objects: 1E 01022-7219,ISM: supernova remnants,Methods: miscellaneous,Stars: neutron},
   month = {11},
   title = {E0102-VR: exploring the scientific potential of Virtual Reality for observational astrophysics},
   url = {http://arxiv.org/abs/1911.04500},
   year = {2019},
}
@article{Marchetti2020,
   abstract = {We present the beta release of iDaVIE-v, a new Virtual Reality software for data cube exploration. The beta release of iDaVIE-v (immersive Data Visualisation Interactive Explorer for volumetric rendering) is planned for release in early 2021. iDaVIE-v has been developed through the Unity game engine using the SteamVR plugin and is compatible with all commercial headsets. It allows the visualization, exploration and interaction of data for scientific analysis. Originally developed to serve the Hi Radio Astronomy community for Hi source identification, the software has now completed the alpha testing phase and is already showing capabilities that will serve the broader astronomical community and more. iDaVIE-v has been developed at the IDIA Visualisation Lab (IVL) based at the University of Cape Town in collaboration with the Italian National Institute for Astrophysics (INAF) in Catania.},
   author = {Lucia Marchetti and Thomas H. Jarrett and Angus Comrie and Alexander K. Sivitilli and Fabio Vitello and Ugo Becciani and A. R. Taylor},
   issn = {23318422},
   journal = {arXiv},
   pages = {1-4},
   title = {iDaVIE-v: Immersive data visualisation interactive explorer for volumetric rendering},
   year = {2020},
}
@article{Iskandar2020,
   abstract = {<p>Mandatory servers for universal applications that is accessible to number of users may be a deterrent for the corporation and excessive for small applications even though it could bring the compatibility advantages. Knowing that demand of web application increases to provide convenience and ease of use to the users, client side rendering comes to create software more fast and efficient. It has been done by redirecting the request towards an HTML file then the server will give messages without any content or a loading screen until the device takes all JavaScript to allow the browser compiling everything before displaying the content. Therefore, the purpose of this paper is to analyse the comparison between client side and server side method in the respect of technical aspects in term of first content paint, speed index, time to interactive, first meaningful paint, first idle CPU and estimated input latency that present better performance with 2.1s, 2.0s, 2.2s, 2.1s, 2.2s and 20ms respectively on server side. It also provide better result based on Google Audit with 100% performance, 48% accessibility, 93% best practice and 89% of search engine optimization (SEO).</p>},
   author = {Taufan Fadhilah Iskandar and Muharman Lubis and Tien Fabrianti Kusumasari and Arif Ridho Lubis},
   doi = {10.1088/1757-899X/801/1/012136},
   issn = {1757-8981},
   issue = {1},
   journal = {IOP Conference Series: Materials Science and Engineering},
   keywords = {affordable,fast,flexible,open access,proceedings,template},
   month = {5},
   pages = {012136},
   title = {Comparison between client-side and server-side rendering in the web development},
   volume = {801},
   url = {https://iopscience.iop.org/article/10.1088/1757-899X/801/1/012136},
   year = {2020},
}
@article{Dumitrescu2019,
   abstract = {<p>In this paper, a set of techniques used for downsampling and upsampling of 2D images is analyzed on various image datasets. The comparison takes into account a significant number of interpolation kernels, their parameters, and their algebraical form, focusing mostly on linear interpolation methods with symmetric kernels. The most suitable metrics for measuring the performance of upsampling and downsampling filters’ combinations are presented, discussing their strengths and weaknesses. A test benchmark is proposed, and the obtained results are analyzed with respect to the presented metrics, offering explanations about specific filter behaviors in general, or just in certain circumstances. In the end, a set of filters and parameters recommendations is offered based on extensive testing on carefully selected image datasets. The entire research is based on the study of a large set of research papers and on a solid discussion of the underlying signal processing theory.</p>},
   author = {Dumitrescu and Boiangiu},
   doi = {10.3390/computers8020030},
   issn = {2073-431X},
   issue = {2},
   journal = {Computers},
   keywords = {downsampling,filters,image processing,interpolation,signal processing,upsampling},
   month = {4},
   pages = {30},
   title = {A Study of Image Upsampling and Downsampling Filters},
   volume = {8},
   url = {https://www.mdpi.com/2073-431X/8/2/30},
   year = {2019},
}
@article{Lowe2020,
   abstract = {The continual growth of big data necessitates efficient ways of analysing these large datasets. Data visualisation and visual analytics has been identified as a key tool in big data analysis because they draw on the human visual and cognitive capabilities to analyse data quickly, intuitively and interactively. However, current visualisation tools and visual analytical systems fall short of providing a seamless user experience and several improvements could be made to current commercially available visualisation tools. By conducting a systematic literature review, requirements of visualisation tools were identified and categorised into six groups: dimensionality reduction, data reduction, scalability and readability, interactivity, fast retrieval of results, and user assistance. The most common themes found in the literature were dimensionality reduction and interactive data exploration.},
   author = {Joy Lowe and Machdel Matthee},
   doi = {10.1007/978-3-030-44999-5_39/FIGURES/3},
   isbn = {9783030449988},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Big data visualisation,Visual analytics,Visualisation tools},
   pages = {469-480},
   publisher = {Springer},
   title = {Requirements of Data Visualisation Tools to Analyse Big Data: A Structured Literature Review},
   volume = {12066 LNCS},
   url = {https://link-springer-com.ezproxy.uct.ac.za/chapter/10.1007/978-3-030-44999-5_39},
   year = {2020},
}
@inproceedings{Li2016,
   abstract = {With the advent of the "big data" era, there are unprecedented opportunities and challenges to explore complex and large datasets. In the paper, we introduce Polyspector™, a web-based interactive visualization platform optimized for interactive visual analysis with two distinguishing features. Firstly, a visualization-specific database engine based on pixel-aware aggregation is implemented to generate views of hundreds of millions of data items within a second even with an off-the-shelf PC. Secondly, a novel deep-linking mechanism, combined with the pixel-aware aggregation, is exploited to realize interactive visual analysis interfaces such as zooming, overview + detail, context + focus etc.},
   author = {Xinxiao Li and Akira Kuroda and Hidenori Matsuzaki},
   city = {New York, NY, USA},
   doi = {10.1145/2984751.2985720},
   isbn = {9781450345316},
   journal = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
   keywords = {Author Keywords Visualization,I36 Interaction techniques,Interactive Visual Analysis ACM Classification Keywords H52 Graphical user interfaces (GUI)},
   month = {10},
   pages = {109-111},
   publisher = {ACM},
   title = {Polyspector™},
   url = {https://dl.acm.org/doi/10.1145/2984751.2985720},
   year = {2016},
}
@article{Masiane2019,
   abstract = {Creating an interactive, accurate, and low-latency big data visualisation is challenging due to the volume, variety, and velocity of the data. Visualisation options range from visualising the entire big dataset, which could take a long time and be taxing to the system, to visualising a small subset of the dataset, which could be fast and less taxing to the system but could also lead to a less-beneficial visualisation as a result of information loss. The main research questions investigated by this work are what effect sampling has on visualisation insight and how to provide guidance to users in navigating this trade-off. To investigate these issues, we study an initial case of simple estimation tasks on histogram visualisations of sampled big data, in hopes that these results may generalise. Leveraging sampling, we generate subsets of large datasets and create visualisations for a crowd-sourced study involving a simple cognitive visualisation task. Using the results of this study, we quantify insight, sampling, visualisation, and perception error in comparison to the full dataset. We use these results to model the relationship between sample size and insight error, and we propose the use of our model to guide big data visualisation sampling. ARTICLE HISTORY},
   author = {Moeti M Masiane and Anne Driscoll and Wuchun Feng and John Wenskovitch and Chris North},
   doi = {10.1080/0144929X.2019.1616223},
   issn = {1362-3001},
   keywords = {Visualisation,big data,error,insight,sampling},
   title = {Towards insight-driven sampling for big data visualisation},
   url = {https://doi.org/10.1080/0144929X.2019.1616223},
   year = {2019},
}
@inproceedings{Abidi2017,
   abstract = {1 ABSTRACT Remote visualization has emerged as a necessary tool in the analysis of big data. High-performance computing clusters can provide several benefits in scaling to larger data sizes, from parallel file systems to larger RAM profiles to parallel computation among many CPUs and GPUs. For scalable data visualization, remote visualization tools and infrastructure is critical where only pixels and interaction events are sent over the network instead of the data. In this paper, we present our pipeline using VirtualGL, TurboVNC, and ParaView to render over 40 million points using remote HPC clusters and project over 26 million pixels in a CAVE-style system. We benchmark the system by varying the video stream compression parameters supported by TurboVNC and establish some best practices for typical usage scenarios. This work will help research scientists and academicians in scaling their big data visualizations for remote, real-time interaction.},
   author = {Faiz Abidi and Nicholas Polys and Srijith Rajamohan and Lance Arsenault and Ayat Mohammed},
   doi = {10.22360/SpringSim.2018.HPC.006},
   isbn = {9781510860162},
   journal = {High Performance Computing Symposium (HPC 2018)},
   keywords = {CAVE,HPC,ParaView,Remote rendering,big data},
   publisher = {Society for Modeling and Simulation International (SCS)},
   title = {Remote High Performance Visualization of Big Data for Immersive Science},
   url = {https://dl.acm.org/citation.cfm?id=3213074},
   year = {2017},
}
@article{Zhao2017,
   abstract = {When using data-mining tools to analyze big data, users often need tools to support the understanding of individual data attributes and control the analysis progress. This requires the integration of data-mining algorithms with interactive tools to manipulate data and analytical process. This is where visual analytics can help. More than simple visualization of a dataset or some computation results, visual analytics provides users an environment to iteratively explore different inputs or parameters and see the corresponding results. In this research, we explore a design of progressive visual analytics to support the analysis of categorical data with a data-mining algorithm, Apriori. Our study focuses on executing data mining techniques step-by-step and showing intermediate result at every stage to facilitate sense-making. Our design, called Pattern Discovery Tool, targets for a medical dataset. Starting with visualization of data properties and immediate feedback of users' inputs or adjustments, Pattern Discovery Tool could help users detect interesting patterns and factors effectively and efficiently. Afterward, further analyses such as statistical methods could be conducted to test those possible theories.},
   author = {Hanqing Zhao and Huijun Zhang and Yan Liu and Yongzhen Zhang and Xiaolong (Luke) Zhang},
   doi = {10.1016/j.jvlc.2017.05.004},
   issn = {1045926X},
   journal = {Journal of Visual Languages & Computing},
   keywords = {Categorical data analysis,Progressive,Visual analytics},
   month = {12},
   pages = {42-49},
   title = {Pattern discovery: A progressive visual analytic design to support categorical data analysis},
   volume = {43},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S1045926X16301732},
   year = {2017},
}
@inproceedings{Glueck2014,
   abstract = {We present Panelrama, a web-based framework for the construction of applications using distributed user interfaces (DUIs). Our implementation provides developers with low migration costs through built-in mechanisms for the synchronization of a UI state, requiring minimal changes to existing languages. Additionally, we describe a solution to categorize device characteristics and dynamically change UI allocation to best-fit devices. We illustrate the use of Panelrama through three sample applications which demonstrate its support for known interaction methods, we also present the results of a developer study, which validates our belief that cross-device application experiences can be easily implemented using our framework.},
   author = {Michael Glueck and Azam Khan and Daniel J. Wigdor},
   city = {New York, NY, USA},
   doi = {10.1145/2556288.2557195},
   isbn = {9781450324731},
   journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
   keywords = {Distributed user interfaces,Multi-device environments},
   month = {4},
   pages = {561-570},
   publisher = {ACM},
   title = {Dive in!},
   url = {https://dl.acm.org/doi/10.1145/2556288.2557195},
   year = {2014},
}
@inproceedings{Rosenbaum2009,
   abstract = {Progressive refinement is commonly understood as a means to solve problems imposed by limited system resources. In this publication, we apply this technology as a novel approach for information presentation and device adaptation. The progressive refinement is able to handle different kinds of data and consists of innovative ideas to overcome the multiple issues imposed by large data volumes. The key feature is the mature use of multiple incremental previews to the data. This leads to a temporal deskew of the information to be presented and provides a causal flow in terms of a tour-through-the-data. Such a presentation is scalable leading to a significantly simplified adaptation to the available resources, short response times, and reduced visual clutter. Due to its rather beneficial properties and feedback we received from first implementations, we state that there is high potential of progressive refinement far beyond its currently addressed application context. © 2009 SPIE-IS&T.},
   author = {René Rosenbaum and Heidrun Schumann},
   doi = {10.1117/12.810501},
   editor = {Katy Börner and Jinah Park},
   issn = {0277786X},
   journal = {Visualization and Data Analysis 2009},
   month = {1},
   pages = {72430I},
   publisher = {SPIE},
   title = {Progressive refinement: more than a means to overcome limited bandwidth},
   volume = {7243},
   url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.810501},
   year = {2009},
}
@article{Muhlbacher2014,
   abstract = {An increasing number of interactive visualization tools stress the integration with computational software like MATLAB and R to access a variety of proven algorithms. In many cases, however, the algorithms are used as black boxes that run to completion in isolation which contradicts the needs of interactive data exploration. This paper structures, formalizes, and discusses possibilities to enable user involvement in ongoing computations. Based on a structured characterization of needs regarding intermediate feedback and control, the main contribution is a formalization and comparison of strategies for achieving user involvement for algorithms with different characteristics. In the context of integration, we describe considerations for implementing these strategies either as part of the visualization tool or as part of the algorithm, and we identify requirements and guidelines for the design of algorithmic APIs. To assess the practical applicability, we provide a survey of frequently used algorithm implementations within R regarding the fulfillment of these guidelines. While echoing previous calls for analysis modules which support data exploration more directly, we conclude that a range of pragmatic options for enabling user involvement in ongoing computations exists on both the visualization and algorithm side and should be used.},
   author = {Thomas Muhlbacher and Harald Piringer and Samuel Gratzl and Michael Sedlmair and Marc Streit},
   doi = {10.1109/TVCG.2014.2346578},
   issn = {10772626},
   issue = {12},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   keywords = {Visual analytics infrastructures,integration,interactive algorithms, user involvement,problem subdivision},
   month = {12},
   pages = {1643-1652},
   pmid = {26356878},
   publisher = {IEEE Computer Society},
   title = {Opening the black box: Strategies for increased user involvement in existing algorithm implementations},
   volume = {20},
   year = {2014},
}
@article{Ali2016,
   abstract = {With the explosion of social media sites and proliferation of digital computing devices and Internet access, massive amounts of public data is being generated on a daily basis. Efficient techniques/ algorithms to analyze this massive amount of data can provide near real-time information about emerging trends and provide early warning in case of an imminent emergency (such as the outbreak of a viral disease). In addition, careful mining of these data can reveal many useful indicators of socioeconomic and political events, which can help in establishing effective public policies. The focus of this study is to review the application of big data analytics for the purpose of human development. The emerging ability to use big data techniques for development (BD4D) promises to revolutionalize healthcare, education, and agriculture; facilitate the alleviation of poverty; and help to deal with humanitarian crises and violent conflicts. Besides all the benefits, the large-scale deployment of BD4D is beset with several challenges due to the massive size, fast-changing and diverse nature of big data. The most pressing concerns relate to efficient data acquisition and sharing, establishing of context (e.g., geolocation and time) and veracity of a dataset, and ensuring appropriate privacy. In this study, we provide a review of existing BD4D work to study the impact of big data on the development of society. In addition to reviewing the important works, we also highlight important challenges and open issues.},
   author = {Anwaar Ali and Junaid Qadir and Raihan ur Rasool and Arjuna Sathiaseelan and Andrej Zwitter and Jon Crowcroft},
   doi = {10.1186/S41044-016-0002-4/TABLES/2},
   issue = {1},
   journal = {Big Data Analytics},
   keywords = {Algorithm Analysis and Problem Complexity,Big Data,Computational Biology/Bioinformatics,Data Mining and Knowledge Discovery,Data Structures,Data Structures and Information Theory},
   month = {12},
   pages = {1-24},
   publisher = {Springer Science and Business Media LLC},
   title = {Big data for development: applications and techniques},
   volume = {1},
   url = {https://bdataanalytics.biomedcentral.com/articles/10.1186/s41044-016-0002-4},
   year = {2016},
}
@article{Tufte1983,
   abstract = {The subject of this book is on statistical graphics, charts, tables. It deals with the theory and practice in the design of data graphics, and includes 250 illustrations of the best (and a few of the worst) statistical graphics, with detailed analysis of how to display data for precise, effective, quick analysis. Also offered is information on the design of the high-resolution displays, small multiples, editing and improving graphics, and the data-ink ratio. Time-series, relational graphics, data maps, multivariate designs, as well as detection of graphical deception: design variation vs. data variation, and sources of deception are discussed. Information on aesthetics and data graphical displays is included. Graphical Practice. Graphical excellence ; Graphical integrity ; Sources of graphical integrity and sophistication. -- Theory of Data Graphics. Data-ink and graphical redesign ; Chartjunk: vibrations, grids, and ducks ; Data-ink maximization and graphical design ; Multifunctioning graphical elements ; Data density and small multiples ; Aesthetics and technique in data graphical design.},
   author = {Edward R. Tufte},
   isbn = {978-0-9613921-0-9},
   keywords = {BibTeX,bookmarks,collaborative tagging,folksonomy,knowledge management,publication management},
   pages = {197},
   publisher = {Graphics Press},
   title = {The visual display of quantitative information},
   year = {1983},
}
@article{Yang2017,
   abstract = {Big Data has emerged in the past few years as a new paradigm providing abundant data and opportunities to improve and/or enable research and decision-support applications with unprecedented value for digital earth applications including business, sciences and engineering. At the same time, Big Data presents challenges for digital earth to store, transport, process, mine and serve the data. Cloud computing provides fundamental support to address the challenges with shared computing resources including computing, storage, networking and analytical software; the application of these resources has fostered impressive Big Data advancements. This paper surveys the two frontiers–Big Data and cloud computing–and reviews the advantages and consequences of utilizing cloud computing to tackling Big Data in the digital earth and relevant science domains. From the aspects of a general introduction, sources, challenges, technology status and research opportunities, the following observations are offered: (i) cloud computing and Big Data enable science discoveries and application developments; (ii) cloud computing provides major solutions for Big Data; (iii) Big Data, spatiotemporal thinking and various application domains drive the advancement of cloud computing and relevant technologies with new requirements; (iv) intrinsic spatiotemporal principles of Big Data and geospatial sciences provide the source for finding technical and theoretical solutions to optimize cloud computing and processing Big Data; (v) open availability of Big Data and processing capability pose social challenges of geospatial significance and (vi) a weave of innovations is transforming Big Data into geospatial research, engineering and business values. This review introduces future innovations and a research agenda for cloud computing supporting the transformation of the volume, velocity, variety and veracity into values of Big Data for local to global digital earth science and applications.},
   author = {Chaowei Yang and Qunying Huang and Zhenlong Li and Kai Liu and Fei Hu},
   doi = {10.1080/17538947.2016.1239771},
   issn = {17538955},
   issue = {1},
   journal = {International Journal of Digital Earth},
   keywords = {CyberGIS,Smart cities,Spatiotemporal computing,digital earth,geoinformatics,geospatial cyberinfrastructure},
   month = {1},
   pages = {13-53},
   publisher = {Taylor and Francis Ltd.},
   title = {Big Data and cloud computing: innovation opportunities and challenges},
   volume = {10},
   year = {2017},
}
@article{Fisher2012,
   abstract = {Queries over large scale (petabyte) data bases often mean waiting overnight for a result to come back. Scale costs time. Such time also means that potential avenues of exploration are ignored because the costs are perceived to be too high to run or even propose them. With sampleAction we have explored whether interaction techniques to present query results running over only incremental samples can be presented as sufficiently trustworthy for analysts both to make closer to real time decisions about their queries and to be more exploratory in their questions of the data. Our work with three teams of analysts suggests that we can indeed accelerate and open up the query process with such incremental visualizations. Copyright 2012 ACM.},
   author = {Danyel Fisher and Igor Popov and Steven M. Drucker and M. C. Schraefel},
   doi = {10.1145/2207676.2208294},
   isbn = {9781450310154},
   journal = {Conference on Human Factors in Computing Systems - Proceedings},
   keywords = {Exploratory data analysis,Incremental visualizations,Large data,Online aggregation},
   pages = {1673-1682},
   title = {Trust me, i'm partially right: Incremental visualization lets analysts explore large datasets faster},
   url = {https://dl-acm-org.ezproxy.uct.ac.za/doi/10.1145/2207676.2208294},
   year = {2012},
}
@article{Yi2007,
   abstract = {Even though interaction is an important part of information visualization (Infovis), it has garnered a relatively low level of attention from the Infovis community. A few frameworks and taxonomies of Infovis interaction techniques exist, but they typically focus on low-level operations and do not address the variety of benefits interaction provides. After conducting an extensive review of Infovis systems and their interactive capabilities, we propose seven general categories of interaction techniques widely used in Infovis: 1) Select, 2) Explore, 3) Reconfigure, 4) Encode, 5) Abstract/Elaborate, 6) Filter, and 7) Connect. These categories are organized around a user's intent while interacting with a system rather than the low-level interaction techniques provided by a system. The categories can act as a framework to help discuss and evaluate interaction techniques and hopefully lay an initial foundation toward a deeper understanding and a science of interaction. © 2007 IEEE.},
   author = {Ji Soo Yi and Youn Ah Kang and John T. Stasko and Julie A. Jacko},
   doi = {10.1109/TVCG.2007.70515},
   issn = {10772626},
   issue = {6},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   keywords = {Information visualization,Interaction,Interaction techniques,Taxonomy,Visual analytics},
   month = {11},
   pages = {1224-1231},
   title = {Toward a deeper understanding of the role of interaction in information visualization},
   volume = {13},
   year = {2007},
}
@article{MacKenzie1993,
   abstract = {The sources of lag (the delay input action and output response) and its effects on human performance are discussed. We measured the effects in a study of target acquisition using the classic Fitts' law paradigm with the addition of four lag conditions. At the highest lag tested (225 ms) movement times and error rates increased by 64% and 214% respectively, compared to the zero lag condition. We propose a model according to which lag should have a multiplicative effect on Fitts' index of difficulty. The model accounts for 94% of the variance and is better than alternative models which propose only an additive effect for lag. The implications for the design of virtual reality systems are discussed.},
   author = {I. Scott MacKenzie and Colin Ware},
   doi = {10.1145/169059.169431},
   isbn = {5198244120},
   journal = {Conference on Human Factors in Computing Systems - Proceedings},
   keywords = {Fitts' law,Human performance modeling,feedback delay,lag,speed-accuracy tradeoff,virtual reaIity},
   pages = {488-493},
   publisher = {Publ by ACM},
   title = {Lag as a determinant of human performance in interactive systems},
   url = {https://dl-acm-org.ezproxy.uct.ac.za/doi/10.1145/169059.169431},
   year = {1993},
}
@book{Tukey1977,
  added-at = {2009-10-28T04:42:52.000+0100},
  author = {Tukey, John W.},
  biburl = {https://www.bibsonomy.org/bibtex/2e0c6e346a4594eb8f0bc71a6e65366fb/jwbowers},
  citeulike-article-id = {107137},
  date-added = {2007-09-03 22:45:16 -0500},
  date-modified = {2007-09-03 22:45:16 -0500},
  interhash = {69fff218408c5eda560e267ff0099dec},
  intrahash = {e0c6e346a4594eb8f0bc71a6e65366fb},
  keywords = {eda statistics},
  publisher = {Addison-Wesley},
  timestamp = {2009-10-28T04:43:20.000+0100},
  title = {Exploratory Data Analysis},
  year = 1977
}
@article{Becker1987,
   abstract = {Dynamic graphical methods have two important properties: direct manipulation of graphical elements on a computer graphics screen and virtually instantaneous change of the elements. The data analyst takes an action through manual manipulation of an input device and something happens in real time on the screen. These computing capabilities provide a new medium for the invention of graphical methods for data analysis. A collection of such methods-identification, deletion, linking, brushing, scaling, rotation, and dynamic parameter control-is reviewed. Those who develop dynamic methods must deal with a number of computer hardware and software issues, because for a dynamic method to work there must be a sufficiently fast flow of information along the channel that starts with the analyst’s input and ends with the changed graph. Furthermore, for a dynamic method to be useful, the visual and manual tasks must be easy to perform. Several computing issues dealing with speed and ease of use-system bandwidth, input and output devices, low-level algorithms, device independent graphics, and data analysis environments-are reviewed. © 1987, Institute of Mathematical Statistics. All Rights Reserved.},
   author = {Richard A. Becker and William S. Cleveland and Allan R. Wilks},
   doi = {10.1214/SS/1177013104},
   issn = {08834237},
   issue = {4},
   journal = {Statistical Science},
   keywords = {Brushing,Computer graphics,Hardware,Multivariate analysis,Rotation,Software,Statistical graphics},
   pages = {355-383},
   title = {Dynamic graphics for data analysis},
   volume = {2},
   url = {https://www.researchgate.net/publication/38363636_Dynamic_Graphics_for_Data_Analysis},
   year = {1987},
}
@article{Hassan2011,
   abstract = {Astronomy is entering a new era of discovery, coincident with the establishment of new facilities for observation and simulation that will routinely generate petabytes of data. While an increasing reliance on automated data analysis is anticipated, a critical role will remain for visualization-based knowledge discovery. We have investigated scientific visualization applications in astronomy through an examination of the literature published during the last two decades. We identify the two most active fields for progress — visualization of large- N particle data and spectral data cubes—discuss open areas of research, and introduce a mapping between astronomical sources of data and data representations used in general-purpose visualization tools. We discuss contributions using high-performance computing architectures (e.g. distributed processing and GPUs), collaborative astronomy visualization, the use of workflow systems to store metadata about visualization parameters, and the use of advanced interaction devices. We examine a number of issues that may be limiting the spread of scientific visualization research in astronomy and identify six grand challenges for scientific visualization research in the Petascale Astronomy Era.},
   author = {Amr Hassan and Christopher J. Fluke},
   doi = {10.1071/AS10031},
   issn = {1323-3580},
   issue = {2},
   journal = {Publications of the Astronomical Society of Australia},
   keywords = {methods: data analysis,techniques: miscellaneous},
   month = {1},
   pages = {150-170},
   title = {Scientific Visualization in Astronomy: Towards the Petascale Astronomy Era},
   volume = {28},
   url = {https://www.cambridge.org/core/product/identifier/S1323358000000771/type/journal_article},
   year = {2011},
}
@book{Kaufman2000,
   abstract = {Summary Volume data are three-dimensional (3D) entities that may have information inside them, may not consist of surfaces and edges, or may be too voluminous to be represented geometrically. Volume visualization is a method of extracting meaningful information from volumetric data using interactive graphics and imaging. It is concerned with volume data representation, modeling, manipulation, and rendering [30.], [32.] and [33.]. Volume data are obtained by sampling, simulation, or modeling techniques. For example, a sequence of 2D slices obtained from magnetic resonance imaging (MRI) or computed tomography (CT) is 3D reconstructed into a volume model and visualized for diagnostic purposes or for planning of treatment or surgery. In many computational fields, as in fluid dynamics, the results of simulation typically running on a supercomputer are often visualized as volume data for analysis and verification. In addition, many traditional geometric computer graphics applications, such as CAD and simulation, as well as applications mixing geometric objects with medical data (see Fig. 1), have exploited the advantages of volume techniques called volume graphics for modeling, manipulation, and visualization [31].},
   author = {Arie E. Kaufman},
   doi = {10.1016/B978-012077790-7/50050-3},
   journal = {Handbook of Medical Imaging},
   month = {1},
   pages = {713-730},
   publisher = {Elsevier},
   title = {Volume Visualization in Medicine},
   url = {https://linkinghub.elsevier.com/retrieve/pii/B9780120777907500503},
   year = {2000},
}
@book{McReynolds2005,
   author = {Tom McReynolds and David Blythe},
   doi = {10.1016/B978-155860659-3.50022-6},
   isbn = {978-1-55860-659-3},
   journal = {Advanced Graphics Programming Using OpenGL},
   month = {1},
   pages = {531-570},
   publisher = {Elsevier},
   title = {Scientific Visualization},
   url = {https://linkinghub.elsevier.com/retrieve/pii/B9781558606593500226},
   year = {2005},
}
@article{Norris1994,
    author = {Norris, Ray},
    year = {1994},
    month = {01},
    pages = {51},
    title = {The Challenge of Astronomical Visualisation},
    volume = {61}
}
@inproceedings{wells1979,
  title={FITS-a flexible image transport system},
  author={Wells, Donald Carson and Greisen, Eric W},
  booktitle={Image processing in astronomy},
  pages={445},
  year={1979}
}

@article{Farr2009,
   abstract = {In virtual worlds, objects fall straight down. By replacing a few lines of code to include Newton's gravity, virtual world software can become an N-body simulation code with visualization included where objects move under their mutual gravitational attraction as stars in a cluster. We report on our recent experience of adding a gravitational N-body simulator to the OpenSim virtual world physics engine. OpenSim is an open-source, virtual world server that provides a 3D immersive experience to users who connect using the popular "Second Life" client software from Linden Labs. With the addition of the N-body simulation engine, which we are calling NEO, short for N-Body Experiments in OpenSim, multiple users can collaboratively create point-mass gravitating objects in the virtual world and then observe the subsequent gravitational evolution of their "stellar" system. We view this work as an experiment examining the suitability of virtual worlds for scientific visualization, and we report on future work to enhance and expand the prototype we have built. We also discuss some standardization and technology issues raised by our unusual use of virtual worlds.},
   author = {Will M. Farr and Piet Hut and Jeff Ames and Adam Johnson},
   isbn = {978-989-704-102-0},
   month = {5},
   pages = {49-58},
   publisher = {UTAD},
   title = {An Experiment in Using Virtual Worlds for Scientific Visualization of Self-Gravitating Systems},
   url = {http://arxiv.org/abs/0905.1066},
   year = {2009},
}
@article{Ferrand2018,
   abstract = {On 21 April 2018, the citizens of Wako, Japan, interacted in a novel way with research being carried out at the Astrophysical Big Bang Laboratory (ABBL) at RIKEN. They were able to explore a model of a supernova and its remnant in an immersive three-dimentional format by using virtual reality (VR) technology. In this article, we explain how this experience was developed and delivered to the public, providing practical tips for and reflecting on the successful organisation of an event of this kind.},
   author = {Gilles Ferrand and Don Warren},
   month = {11},
   title = {Engaging the Public with Supernova and Supernova Remnant Research Using Virtual Reality},
   url = {http://arxiv.org/abs/1811.01542},
   year = {2018},
}
@article{Ferrand2016,
   abstract = {We report on an exploratory project aimed at performing immersive 3D visualization of astronomical data, starting with spectral-line radio data cubes from galaxies. This work is done as a collaboration between the Department of Physics and Astronomy and the Department of Computer Science at the University of Manitoba. We are building our prototype using the 3D engine Unity, because of its ease of use for integration with advanced displays such as a CAVE environment, a zSpace tabletop, or virtual reality headsets. We address general issues regarding 3D visualization, such as: load and convert astronomy data, perform volume rendering on the GPU, and produce physically meaningful visualizations using principles of visual literacy. We discuss some challenges to be met when designing a user interface that allows us to take advantage of this new way of exploring data. We hope to lay the foundations for an innovative framework useful for all astronomers who use spectral line data cubes, and encourage interested parties to join our efforts. This pilot project addresses the challenges presented by frontier astronomy experiments, such as the Square Kilometre Array and its precursors.},
   author = {Gilles Ferrand and Jayanne English and Pourang Irani},
   keywords = {HI emission,data visualization,galaxies,radio astronomy,virtual reality},
   month = {7},
   title = {3D visualization of astronomy data cubes using immersive displays},
   url = {http://arxiv.org/abs/1607.08874},
   year = {2016},
}
@article{Taylor2015,
   abstract = {I present a new FITS viewer designed to explore 3D spectral line data (in particular Hi) and assist with visual source extraction and analysis. Using the artistic software Blender, frelled can visualize even large (~6003 voxels) data sets at high frame rates (≳10f.p.s.) in 3D. Blender's interface enables easy navigation within the 3D environment, and the frelled scripts support world coordinate systems. A variety of tools are included to aid source extraction and analysis, including interactively masking data (using 3D polyhedra of arbitrary complexity), querying NED, calculating the flux in specified volumes, generating contour plots and overlaying optical data. It includes tools to overlay n-body particle data, and multi-volume rendering is supported. The interface is designed to make cataloguing sources as easy as possible and I show that this can be as much as a factor of 50 times faster than using other viewers.},
   author = {R. Taylor},
   doi = {10.1016/j.ascom.2015.10.002},
   issn = {22131337},
   journal = {Astronomy and Computing},
   keywords = {Galaxies: kinematics and dynamics,Radio lines: galaxies,Scientific visualization,Surveys,Visual analytics},
   month = {11},
   pages = {67-79},
   publisher = {Elsevier B.V.},
   title = {Frelled: A realtime volumetric data viewer for astronomers},
   volume = {13},
   url = {http://dx.doi.org/10.1016/j.ascom.2015.10.002 https://linkinghub.elsevier.com/retrieve/pii/S2213133715000955},
   year = {2015},
}
@article{Fedorov2012,
   abstract = {Quantitative analysis has tremendous but mostly unrealized potential in healthcare to support objective and accurate interpretation of the clinical imaging. In 2008, the National Cancer Institute began building the Quantitative Imaging Network (QIN) initiative with the goal of advancing quantitative imaging in the context of personalized therapy and evaluation of treatment response. Computerized analysis is an important component contributing to reproducibility and efficiency of the quantitative imaging techniques. The success of quantitative imaging is contingent on robust analysis methods and software tools to bring these methods from bench to bedside.3D Slicer is a free open-source software application for medical image computing. As a clinical research tool, 3D Slicer is similar to a radiology workstation that supports versatile visualizations but also provides advanced functionality such as automated segmentation and registration for a variety of application domains. Unlike a typical radiology workstation, 3D Slicer is free and is not tied to specific hardware. As a programming platform, 3D Slicer facilitates translation and evaluation of the new quantitative methods by allowing the biomedical researcher to focus on the implementation of the algorithm and providing abstractions for the common tasks of data communication, visualization and user interface development. Compared to other tools that provide aspects of this functionality, 3D Slicer is fully open source and can be readily extended and redistributed. In addition, 3D Slicer is designed to facilitate the development of new functionality in the form of 3D Slicer extensions. In this paper, we present an overview of 3D Slicer as a platform for prototyping, development and evaluation of image analysis tools for clinical research applications. To illustrate the utility of the platform in the scope of QIN, we discuss several use cases of 3D Slicer by the existing QIN teams, and we elaborate on the future directions that can further facilitate development and validation of imaging biomarkers using 3D Slicer. © 2012 Elsevier Inc.},
   author = {Andriy Fedorov and Reinhard Beichel and Jayashree Kalpathy-Cramer and Julien Finet and Jean Christophe Fillion-Robin and Sonia Pujol and Christian Bauer and Dominique Jennings and Fiona Fennessy and Milan Sonka and John Buatti and Stephen Aylward and James V. Miller and Steve Pieper and Ron Kikinis},
   doi = {10.1016/J.MRI.2012.05.001},
   issn = {0730-725X},
   issue = {9},
   journal = {Magnetic Resonance Imaging},
   keywords = {Brain,CT,Cancer imaging,Cancer treatment response,Glioblastima,Head and neck,Image analysis,Imaging biomarkers,MRI,Medical imaging,PET,Prostate,Quantitative imaging,Software tools},
   month = {11},
   pages = {1323-1341},
   pmid = {22770690},
   publisher = {Elsevier},
   title = {3D Slicer as an image computing platform for the Quantitative Imaging Network},
   volume = {30},
   year = {2012},
}
@article{Lan2021,
   abstract = {We present a state-of-the-art report on visualization in astrophysics. We survey representative papers from both astrophysics and visualization and provide a taxonomy of existing approaches based on data analysis tasks. The approaches are classified based on five categories: data wrangling, data exploration, feature identification, object reconstruction, as well as education and outreach. Our unique contribution is to combine the diverse viewpoints from both astronomers and visualization experts to identify challenges and opportunities for visualization in astrophysics. The main goal is to provide a reference point to bring modern data analysis and visualization techniques to the rich datasets in astrophysics.},
   author = {Fangfei Lan and Michael Young and Lauren Anderson and Anders Ynnerman and Alexander Bock and Michelle A. Borkin and Angus G. Forbes and Juna A. Kollmeier and Bei Wang},
   doi = {10.1111/cgf.14332},
   issn = {0167-7055},
   issue = {3},
   journal = {Computer Graphics Forum},
   month = {6},
   pages = {635-663},
   publisher = {John Wiley & Sons, Ltd},
   title = {Visualization in Astrophysics: Developing New Methods, Discovering Our Universe, and Educating the Earth},
   volume = {40},
   url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.14332},
   year = {2021},
}
@article{Taylor2016,
   abstract = {Astronomical data does not always use Cartesian coordinates. Both all-sky observational data and simulations of rotationally symmetric systems, such as accretion and protoplanetary discs, may use spherical polar or other coordinate systems. Standard displays rely on Cartesian coordinates, but converting non-Cartesian data into Cartesian format causes distortion of the data and loss of detail. I here demonstrate a method using standard techniques from computer graphics that avoids these problems with 3D data in arbitrary coordinate systems. The method adds minimum computational cost to the display process and is suitable for both realtime, interactive content and producing fixed rendered images and videos. Proof-of-concept code is provided which works for data in spherical polar coordinates.},
   author = {Rhys Taylor},
   doi = {10.1088/1538-3873/129/972/028002},
   keywords = {galaxies: kinematics and dynamics,radio lines: galaxies,scientific visualization,surveys,visual analytics},
   month = {11},
   title = {Visualising three-dimensional volumetric data with an arbitrary coordinate system},
   url = {http://arxiv.org/abs/1611.02517 http://dx.doi.org/10.1088/1538-3873/129/972/028002},
   year = {2016},
}
@article{Kent2013,
   abstract = {Astronomical data take on a multitude of forms -- catalogs, data cubes, images, and simulations. The availability of software for rendering high-quality three-dimensional graphics lends itself to the paradigm of exploring the incredible parameter space afforded by the astronomical sciences. The software program Blender gives astronomers a useful tool for displaying data in a manner used by three-dimensional (3D) graphics specialists and animators. The interface to this popular software package is introduced with attention to features of interest in astronomy. An overview of the steps for generating models, textures, animations, camera work, and renders is outlined. An introduction is presented on the methodology for producing animations and graphics with a variety of astronomical data. Examples from sub-fields of astronomy with different kinds of data are shown with resources provided to members of the astronomical community. An example video showcasing the outlined principles and features is provided along with scripts and files for sample visualizations.},
   author = {Brian R. Kent},
   doi = {10.1086/671412},
   issn = {00046280},
   issue = {928},
   journal = {Publications of the Astronomical Society of the Pacific},
   month = {6},
   pages = {731-748},
   title = {Visualizing Astronomical Data with Blender},
   volume = {125},
   url = {http://iopscience.iop.org/article/10.1086/671412},
   year = {2013},
}
@article{Goodman2012,
   abstract = {Astronomical researchers often think of analysis and visualization as separate tasks. In the case of high-dimensional data sets, though, interactive exploratory data visualization can give far more insight than an approach where data processing and statistical analysis are followed, rather than accompanied, by visualization. This paper attempts to charts a course toward "linked view" systems, where multiple views of high-dimensional data sets update live as a researcher selects, highlights, or otherwise manipulates, one of several open views. For example, imagine a researcher looking at a 3D volume visualization of simulated or observed data, and simultaneously viewing statistical displays of the data set's properties (such as an x-y plot of temperature vs. velocity, or a histogram of vorticities). Then, imagine that when the researcher selects an interesting group of points in any one of these displays, that the same points become a highlighted subset in all other open displays. Selections can be graphical or algorithmic, and they can be combined, and saved. For tabular (ASCII) data, this kind of analysis has long been possible, even though it has been under-used in astronomy. The bigger issue for astronomy and other "high-dimensional" fields, though, is that no extant system allows for full integration of images and data cubes within a linked-view environment. The paper concludes its history and analysis of the present situation with suggestions that look toward cooperatively-developed open-source modular software as a way to create an evolving, flexible, high-dimensional, linked-view visualization environment useful in astrophysical research.},
   author = {A.A. Goodman},
   doi = {10.1002/asna.201211705},
   issn = {00046337},
   issue = {5-6},
   journal = {Astronomische Nachrichten},
   keywords = {clouds-methods,cosmology,data analysis-techniques,image processing-techniques,large-scale structure-ISM,radial velocities},
   month = {6},
   pages = {505-514},
   title = {Principles of high-dimensional data visualization in astronomy},
   volume = {333},
   url = {https://onlinelibrary.wiley.com/doi/10.1002/asna.201211705},
   year = {2012},
}
@misc{Wittenbrink2000,
   abstract = {There is a wide range of devices and scientific simulation generating volumetric data. Visualizing such data, ranging from regular data sets to scattered data, is a challenging task. This course will give an introduction to the volume rendering transport theory and the involved issues such as interpolation, illumination, classification and others. Different volume rendering techniques will be presented illustrating their fundamental features and differences as well as their limitations. Furthermore, acceleration techniques will be presented including pure software optimizations as well as utilizing special purpose hardware as VolumePro but also dedicated hardware such as polygon graphics subsystems.},
   author = {Craig M Wittenbrink and M Meißner and H Pfister and R Westermann and C M Wittenbrink},
   title = {Volume visualization and volume rendering techniques},
   url = {https://www.researchgate.net/publication/228557366},
   year = {2000},
}
@article{Borne2009,
   abstract = {Data volumes from multiple sky surveys have grown from gigabytes into terabytes during the past decade, and will grow from terabytes into tens (or hundreds) of petabytes in the next decade. This exponential growth of new data both enables and challenges effective astronomical research, requiring new approaches. Thus far, astronomy has tended to address these challenges in an informal and ad hoc manner, with the necessary special expertise being assigned to e-Science or survey science. However, we see an even wider scope and therefore promote a broader vision of this data-driven revolution in astronomical research. For astronomy to effectively cope with and reap the maximum scientific return from existing and future large sky surveys, facilities, and data-producing projects, we need our own information science specialists. We therefore recommend the formal creation, recognition, and support of a major new discipline, which we call Astroinformatics. Astroinformatics includes a set of naturally-related specialties including data organization, data description, astronomical classification taxonomies, astronomical concept ontologies, data mining, machine learning, visualization, and astrostatistics. By virtue of its new stature, we propose that astronomy now needs to integrate Astroinformatics as a formal sub-discipline within agency funding plans, university departments, research programs, graduate training, and undergraduate education. Now is the time for the recognition of Astroinformatics as an essential methodology of astronomical research. The future of astronomy depends on it.},
   author = {Kirk D. Borne},
   month = {9},
   title = {Astroinformatics: A 21st Century Approach to Astronomy},
   url = {http://arxiv.org/abs/0909.3892},
   year = {2009},
}
@article{Naiman2016,
   abstract = {The rapid growth in scale and complexity of both computational and
observational astrophysics over the past decade necessitates efficient and
intuitive methods for examining and visualizing large datasets. Here, I present
\{\it AstroBlend\}, an open-source Python library for use within the three
dimensional modeling software, \{\it Blender\}. While \{\it Blender\} has been a
popular open-source software among animators and visual effects artists, in
recent years it has also become a tool for visualizing astrophysical datasets.
\{\it AstroBlend\} combines the three dimensional capabilities of \{\it Blender\}
with the analysis tools of the widely used astrophysical toolset, \{\it yt\}, to
afford both computational and observational astrophysicists the ability to
simultaneously analyze their data and create informative and appealing
visualizations. The introduction of this package includes a description of
features, work flow, and various example visualizations. A website -
www.astroblend.com - has been developed which includes tutorials, and a gallery
of example images and movies, along with links to downloadable data, three
dimensional artistic models, and various other resources.},
   author = {J.P. Naiman},
   doi = {10.1016/j.ascom.2016.02.002},
   issn = {22131337},
   journal = {Astronomy and Computing},
   keywords = {Methods: numerical,Miscellaneous,numerical},
   month = {4},
   pages = {50-60},
   publisher = {Elsevier},
   title = {AstroBlend: An astrophysical visualization package for Blender},
   volume = {15},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S2213133716300117},
   year = {2016},
}
@article{Comrie2021,
   author = {Angus Comrie and Kuo-Song Wang and Shou-Chieh Hsu and Anthony Moraghan and Pamela Harris and Qi Pang and Adrianna Pińska and Cheng-Chin Chiang and Tien-Hao Chang and Yu-Hsuan Hwang and Hengtai Jan and Ming-Yi Lin and Rob Simmonds},
   doi = {10.5281/ZENODO.4905459},
   keywords = {data visualisation,image cube,radio astronomy,visual analytics},
   month = {6},
   title = {CARTA: The Cube Analysis and Rendering Tool for Astronomy},
   url = {https://zenodo.org/record/4905459},
   year = {2021},
}
@inbook{Ahrens2005,
   author = {James Ahrens and Berk Geveci and Charles Law},
   doi = {10.1016/B978-012387582-2/50038-1},
   journal = {Visualization Handbook},
   pages = {717-731},
   publisher = {Elsevier},
   title = {ParaView: An End-User Tool for Large-Data Visualization},
   url = {https://linkinghub.elsevier.com/retrieve/pii/B9780123875822500381},
   year = {2005},
}
@article{Blum2021,
   abstract = {In this time of pandemic, the world has turned to Internet-based, RTC (realtime communication) as never before. The number of RTC products has, over the past decade, exploded in large part because of cheaper high-speed network access and more powerful devices, but also because of an open, royalty-free platform called WebRTC. WebRTC is growing from enabling useful experiences to being essential in allowing billions to continue their work and education, and keep vital human contact during a pandemic. The opportunities and impact that lie ahead for WebRTC are intriguing indeed.},
   author = {Niklas Blum and Serge Lachapelle and Harald Alvestrand},
   doi = {10.1145/3454122.3457587},
   issn = {1542-7730},
   issue = {1},
   journal = {Queue},
   month = {2},
   pages = {77-93},
   title = {WebRTC - Realtime Communication for the Open Web Platform},
   volume = {19},
   url = {https://dl.acm.org/doi/10.1145/3454122.3457587},
   year = {2021},
}
@inbook{Danchilla2012,
   abstract = {There are many WebGL frameworks that are available to abstract away the lower-level application programming interface (API) calls that we have covered in the first six chapters of the book. This abstraction helps to make WebGL development easier and more productive....},
   author = {Brian Danchilla},
   city = {Berkeley, CA},
   doi = {10.1007/978-1-4302-3997-0_7},
   journal = {Beginning WebGL for HTML5},
   pages = {173-203},
   publisher = {Apress},
   title = {Three.js Framework},
   url = {http://link.springer.com/10.1007/978-1-4302-3997-0_7},
   year = {2012},
}
@article{Bikakis2018,
   abstract = {Data visualization and analytics are nowadays one of the corner-stones of Data Science, turning the abundance of Big Data being produced through modern systems into actionable knowledge. Indeed, the Big Data era has realized the availability of voluminous datasets that are dynamic, noisy and heterogeneous in nature. Transforming a data-curious user into someone who can access and analyze that data is even more burdensome now for a great number of users with little or no support and expertise on the data processing part. Thus, the area of data visualization and analysis has gained great attention recently, calling for joint action from different research areas and communities such as information visualization, data management and mining, human-computer interaction, and computer graphics. This article presents the limitations of traditional visualization systems in the Big Data era. Additionally, it discusses the major prerequisites and challenges that should be addressed by modern visualization systems. Finally, the state-of-the-art methods that have been developed in the context of the Big Data visualization and analytics are presented, considering methods from the Data Management and Mining, Information Visualization and Human-Computer Interaction communities},
   author = {Nikos Bikakis},
   doi = {10.1007/978-3-319-63962-8_109-2},
   keywords = {Exploratory data analysis 2 Definition,Information visualization,Interactive visualization,Vi-sual analytics,Visual exploration},
   month = {1},
   publisher = {Springer},
   title = {Big Data Visualization Tools},
   url = {http://arxiv.org/abs/1801.08336 http://dx.doi.org/10.1007/978-3-319-63962-8_109-2},
   year = {2018},
}
@inproceedings{Sneiderman1996,
   abstract = {A useful starting point for designing advanced graphical user interjaces is the Visual lnformation-Seeking Mantra: overview first, zoom and filter, then details on demand. But this is only a starting point in trying to understand the rich and varied set of information visualizations that have been proposed in recent years. This paper offers a task by data type taxonomy with seven data types (one-, two-, three-dimensional datu, temporal and multi-dimensional data, and tree and network data) and seven tasks (overview, Zoom, filter, details-on-demand, relate, history, and extracts). Everything points to the conclusion that the phrase 'the language of art' is more than a loose metaphor, that even to describe the visible world in images we need a developed system of schemata. E. H. Gombrich Art and Illusion, 1959 (p. 7 6) keys), are being pushed aside by newer notions of information gathering, seeking, or visualization and data mining, warehousing, or filtering. While distinctions are subtle, the common goals reach from finding a narrow set of items in a large collection that satisfy a well-understood information need (known-item search) to developing an understanding of unexpected patterns within the collection (browse) (Marchionini, 1995). Exploring information collections becomes increasingly difficult as the volume grows. A page of information is easy to explore, but when the information becomes the size of a book, or library, or even larger, it may be difficult to locate known items or to browse to gain an overview, Designers are just discovering how to use the rapid and high resolution color displays to present large amounts of information in orderly and user-controlled ways. Perceptual psychologists, statisticians, and graphic designers (Berlin, 1983; Cleveland, 1993; Tufte, 1983, 1990) offer valuable guidance about presenting static information, but the opportunity for dynamic displays takes user interface designers well beyond current wisdom.},
   author = {B. Shneiderman},
   doi = {10.1109/VL.1996.545307},
   isbn = {0-8186-7508-X},
   journal = {Proceedings 1996 IEEE Symposium on Visual Languages},
   pages = {336-343},
   publisher = {IEEE Comput. Soc. Press},
   title = {The eyes have it: a task by data type taxonomy for information visualizations},
   url = {http://ieeexplore.ieee.org/document/545307/},
}
@inproceedings{Shneiderman2008,
   abstract = {Database searches are usually performed with query languages and form fill in templates, with results displayed in tabular lists. However, excitement is building around dynamic queries sliders and other graphical selectors for query specification, with results displayed by information visualization techniques. These filtering techniques have proven to be effective for many tasks in which visual presentations enable discovery of relationships, clusters, outliers, gaps, and other patterns. Scaling visual presentations from millions to billions of records will require collaborative research efforts in information visualization and database management to enable rapid aggregation, meaningful coordinated windows, and effective summary graphics. This paper describes current and proposed solutions (atomic, aggregated, and density plots) that facilitate sense-making for interactive visual exploration of billion record data sets.},
   author = {Ben Shneiderman},
   city = {New York, NY, USA},
   doi = {10.1145/1376616.1376618},
   isbn = {9781605581026},
   journal = {Proceedings of the 2008 ACM SIGMOD international conference on Management of data},
   keywords = {ACM Classification Keywords H5 Information interfaces and presentation (eg,HCI) H2 DATABASE MANAGEMENT General Terms: Human Factors Author Keywords: Information visualization,aggregation,coordinated windows,database search,density plots,dynamic queries,user interface},
   month = {6},
   pages = {3-12},
   publisher = {ACM},
   title = {Extreme visualization},
   url = {https://dl.acm.org/doi/10.1145/1376616.1376618},
   year = {2008},
}
@article{Caldarola2017,
   abstract = {In the era of Big Data, a great attention deserves the visualization of large data sets. Among the main phases of the data management's life cycle, i.e., storage, analytics and visualization, the last one is the most strategic since it is close to the human perspective. The huge mine of data becomes a gold mine only if tricky and wise analytics algorithms are executed over the data deluge and, at the same time, the analytic process results are visualized in an effective, efficient and why not impressive way. Not surprisingly, a plethora of tools and techniques have emerged in the last years for Big Data visualization, both as part of DataManagement Systems or as software or plugins specifically devoted to the data visualization. Starting from these considerations, this paper provides a survey of the most used and spread visualization tools and techniques for large data sets, eventually presenting a synoptic of the main functional and non-functional characteristics of the surveyed tools.},
   author = {Enrico G. Caldarola and Antonio M. Rinaldi},
   doi = {10.5220/0006484102960305},
   isbn = {9789897582554},
   journal = {DATA 2017 - Proceedings of the 6th International Conference on Data Science, Technology and Applications},
   keywords = {Big data,Big data analytics,Big data visualization,Graph visualization,Information visualization,Visual analytics},
   pages = {296-305},
   publisher = {SciTePress},
   title = {Big data visualization tools: A survey: The new paradigms, methodologies and tools for large data sets visualization},
   year = {2017},
}
@article{kaufman1996,
  title={Volume visualization},
  author={Kaufman, Arie E},
  journal={ACM Computing Surveys (CSUR)},
  volume={28},
  number={1},
  pages={165--167},
  year={1996},
  publisher={ACM New York, NY, USA}
}
@article{Kaufman1993,
   author = {A. Kaufman and D. Cohen and R. Yagel},
   doi = {10.1109/MC.1993.274942},
   issn = {0018-9162},
   issue = {7},
   journal = {Computer},
   month = {7},
   pages = {51-64},
   title = {Volume graphics},
   volume = {26},
   url = {http://ieeexplore.ieee.org/document/274942/},
   year = {1993},
}
@article{Rosenblum1994,
author = {Rosenblum, L. and Earnshaw, R.A. and Encarnação, José and Kaufman, A. and Klimenko, Stanislav and Nielson, Gregory and Post, Frits and Thalmann, Daniel},
year = {1994},
month = {01},
pages = {},
title = {Scientific Visualization—Advances and Challenges}
}
@inproceedings{keim1997,
  title={Visual techniques for exploring databases},
  author={Keim, Daniel A},
  booktitle={Knowledge Discovery in Databases (KDD'97)},
  year={1997}
}
@article{ertl1999,
title = {Multiresolution and hierarchical methods for the visualization of volume data},
journal = {Future Generation Computer Systems},
volume = {15},
number = {1},
pages = {31-42},
year = {1999},
issn = {0167-739X},
doi = {https://doi.org/10.1016/S0167-739X(98)00046-6},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X98000466},
author = {T. Ertl and R. Westermann and R. Grosso},
keywords = {Multiresolution, Visualization, Volume rendering},
abstract = {As three-dimensional data sets resulting from simulations or measurements become available at ever growing sizes the need for visualization tools which allow the inspection and the analysis of these data sets at interactive rates is increasing. One way to deal with the complexity is the compression of the data in such a way that the number of cells which have to be processed by the visualization mapping is reduced. Since this compression will be lossy, it is up to the user to choose between quality or speed. The decision will usually be made interactively requiring fast access to a complete hierarchy of representations of the data set at various levels of resolution. Two different approaches and visualization algorithms based upon them are presented in this paper: wavelet analysis deriving a hierarchy of coarser representations from the original data set and multilevel finite elements generating successively refined tetrahedral grids from an initially coarse triangulation.}
}
@article{manyika2011,
  title={Big data: The next frontier for innovation, competition, and productivity},
  author={Manyika, James and Chui, Michael and Brown, Brad and Bughin, Jacques and Dobbs, Richard and Roxburgh, Charles and Hung Byers, Angela},
  year={2011},
  publisher={McKinsey \& Company}
}
@inproceedings{merv2011,
  title={Big data. Teradata Magazine Online, Q1},
  author={Merv, A},
  year={2011}
}
@article{Wohlgenannt2020,
   author = {Isabell Wohlgenannt and Alexander Simons and Stefan Stieglitz},
   doi = {10.1007/s12599-020-00658-9},
   issn = {2363-7005},
   issue = {5},
   journal = {Business & Information Systems Engineering},
   keywords = {Augmented reality,Extended reality,Immersive systems,Mixed reality,Virtual reality},
   month = {10},
   pages = {455-461},
   title = {Virtual Reality},
   volume = {62},
   url = {https://link.springer.com/10.1007/s12599-020-00658-9},
   year = {2020},
}
@article{Bowman2007,
    author = {Bowman, Doug and McMahan, Ryan},
    year = {2007},
    month = {08},
    pages = {36 - 43},
    title = {Virtual Reality: How Much Immersion Is Enough?},
    volume = {40},
    journal = {Computer},
    doi = {10.1109/MC.2007.257}
}
@article{Brooks1999,
   author = {F.P. Brooks},
   doi = {10.1109/38.799723},
   issn = {02721716},
   issue = {6},
   journal = {IEEE Computer Graphics and Applications},
   pages = {16-27},
   title = {What's real about virtual reality?},
   volume = {19},
   url = {http://ieeexplore.ieee.org/document/799723/},
   year = {1999},
}
@article{Barfield2000,
    author = {Nash, Eric and Edwards, Gregory and Thompson, Jennifer and Barfield, Woodrow},
    year = {2000},
    month = {05},
    pages = {1-41},
    title = {A Review of Presence and Performance in Virtual Environments},
    volume = {12},
    journal = {Int. J. Hum. Comput. Interaction},
    doi = {10.1207/S15327590IJHC1201_1}
}
@article{Walsh2002,
    author = {Walsh, Kenneth and Pawlowski, Suzanne},
    year = {2002},
    month = {03},
    pages = {},
    title = {Virtual Reality: A Technology in Need of IS Research},
    volume = {8},
    journal = {Communications of the AIS},
    doi = {10.17705/1CAIS.00820}
}
@article{Sanchez2005,
    author = {Sanchez-Vives, Maria and Slater, Mel},
    year = {2005},
    month = {05},
    pages = {332-9},
    title = {From presence to consciousness through virtual reality},
    volume = {6},
    journal = {Nature reviews. Neuroscience},
    doi = {10.1038/nrn1651}
}
@article{Steuer2000,
author = {Steuer, Jonathan},
year = {2000},
month = {07},
pages = {},
title = {Defining Virtual Reality: Dimensions Determining Telepresence},
volume = {42},
journal = {Journal of Communication},
doi = {10.1111/j.1460-2466.1992.tb00812.x}
}
@article{Nilsson2016,
    author = {Nilsson, Niels and Nordahl, Rolf and Serafin, Stefania},
    year = {2016},
    month = {11},
    pages = {108-134},
    title = {Immersion Revisited: A Review of Existing Definitions of Immersion and Their Relation to Different Theories of Presence},
    volume = {12},
    journal = {Human Technology},
    doi = {10.17011/ht/urn.201611174652}
}
@misc{Sutherland1965,
   author = {Ivan E Sutherland},
   title = {The Ultimate Display},
   url = {http://www.cee.hw.ac.uk/courses/5ig2/1/ultimate_display.html},
}
@article{Heilig1994,
  title={United States Patent office: stereoscopic-television apparatus for individual use},
  author={Heilig, Morton L},
  journal={ACM SIGGRAPH Computer Graphics},
  volume={28},
  number={2},
  pages={131--134},
  year={1994},
  publisher={ACM New York, NY, USA}
}
@online{VRDAVisGitHub,
    author = "Michaela van Zyl",
    title = "VRDAVis GitHub repository",
    year = "2022",
    url = "https://github.com/Michaelazzz/VRDAVis",
    urldate = "2024-05-13"
}
@misc{MDN_WebSockets_API,
  author       = {{MDN Web Docs}},
  title        = {WebSockets API},
  year         = 2024,
  url          = {https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API},
  note         = {Accessed: 2024-05-20}
}
@misc{Google_Protocol_Buffers,
  author       = {{Google}},
  title        = {Protocol Buffers},
  year         = 2024,
  url          = {https://developers.google.com/protocol-buffers},
  note         = {Accessed: 2024-05-20}
}
@misc{JSON,
  author       = {{JSON}},
  title        = {Introducing JSON},
  year         = 2024,
  url          = {https://www.json.org/json-en.html},
  note         = {Accessed: 2024-05-20}
}
@misc{WebRTC,
  author       = {{WebRTC}},
  title        = {WebRTC: Real-Time Communication for the Web},
  year         = 2024,
  url          = {https://webrtc.org/},
  note         = {Accessed: 2024-05-20}
}
@misc{fits2idia,
  author       = {CARTAvis Team},
  title        = {fits2idia: FITS to IDIA Image Conversion Tool},
  year         = {2023},
  howpublished = {\url{https://github.com/CARTAvis/fits2idia}},
  note         = {Accessed: 2024-09-19}
}